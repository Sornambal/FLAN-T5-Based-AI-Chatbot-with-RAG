# -*- coding: utf-8 -*-
"""Chatbot Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HWVowtDoCkrop4f6z2smP_d5GoCpaERH

### üß© Step 1: Install Required Python Libraries

This step installs all dependencies used in the project:

- **faiss-cpu** ‚Üí For creating and searching vector indexes efficiently  
- **sentence-transformers** ‚Üí For embedding text into numerical vectors  
- **transformers** ‚Üí For using pretrained text-generation models such as FLAN-T5  
- **PyMuPDF (fitz)** ‚Üí To extract text from PDF files  
- **python-docx** ‚Üí To extract text from Word (.docx) files  

All these tools help us process text, build embeddings, and answer questions using AI.
"""

!pip install faiss-cpu sentence-transformers transformers PyMuPDF python-docx gradio

"""### üß† Step 2: Upload Course Notes

Allows you to upload `.txt`, `.pdf`, or `.docx` files interactively in Colab.  
Once uploaded, the filename is displayed and stored for further processing.

"""

from google.colab import files

print("üìé Please upload your course notes (.txt, .pdf, or .docx)")
uploaded = files.upload()
filename = list(uploaded.keys())[0]
print(f"‚úÖ Uploaded: {filename}")

"""### üìÑ Step 3: Extract Text from Uploaded File

This function reads the uploaded file and extracts text depending on its type:

- **TXT** ‚Üí Reads directly  
- **PDF** ‚Üí Extracts page text using PyMuPDF  
- **DOCX** ‚Üí Extracts paragraph text using python-docx  

Finally, we split the text into smaller chunks (300 characters each) for embedding.

"""

import os
import fitz  # PyMuPDF
from docx import Document

def extract_text_from_file(file_path):
    ext = os.path.splitext(file_path)[-1].lower()
    text = ""

    if ext == ".txt":
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            text = f.read()
    elif ext == ".pdf":
        pdf = fitz.open(file_path)
        for page in pdf:
            text += page.get_text("text")
    elif ext == ".docx":
        doc = Document(file_path)
        for para in doc.paragraphs:
            text += para.text + "\n"
    else:
        raise ValueError("‚ùå Unsupported file type! Upload .txt, .pdf, or .docx")

    return text.strip()

course_text = extract_text_from_file(filename)
print(f"‚úÖ Extracted {len(course_text)} characters of text.")

chunk_size = 300
note_texts = [course_text[i:i+chunk_size] for i in range(0, len(course_text), chunk_size)]
print(f"‚úÖ Split into {len(note_texts)} chunks.")

"""### üßÆ Step 4: Build Vector Embeddings with Sentence-Transformer and FAISS

This cell:
1. Loads `all-MiniLM-L6-v2` for generating text embeddings.  
2. Converts each chunk into a numerical vector.  
3. Builds a FAISS index to search the most relevant chunks quickly.

"""

import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

print("üîÑ Building vector embeddings ‚Ä¶")
embedder = SentenceTransformer("all-MiniLM-L6-v2")

embeddings = embedder.encode(note_texts, show_progress_bar=True)
embeddings = np.array(embeddings).astype('float32')

faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
faiss_index.add(embeddings)
print("‚úÖ FAISS index built successfully.")

"""### ü§ñ Step 5: Load FLAN-T5 Model and Define Helper Functions

Loads `google/flan-t5-base`, a fine-tuned T5 model for question answering.  
Defines functions to:
- **Retrieve context** from FAISS  
- **Generate an answer** using the retrieved text and the model

"""

from transformers import pipeline

print("‚öôÔ∏è Loading FLAN-T5-Base Model ‚Ä¶")
generator = pipeline("text2text-generation", model="google/flan-t5-base")

def generate_answer(query):
    query_vector = embedder.encode([query])
    D, I = faiss_index.search(np.array(query_vector).astype('float32'), k=2)
    context = " ".join([note_texts[i] for i in I[0]])
    prompt = f"Answer the question based on the following notes:\n{context}\n\nQuestion: {query}"
    response = generator(prompt, max_new_tokens=200, truncation=True)
    return response[0]['generated_text'].strip()

"""### üé® Step 6: Build Professional Light-Blue Gradio Interface

Creates a web interface using **Gradio**.

- Background ‚Üí Light Blue `#E8F1FA`  
- Text ‚Üí Dark Black `#1A1A1A`  
- Clean chat window + upload area  
- Two functions:
  - `process_notes()` ‚Üí Rebuilds index if new file uploaded  
  - `ask_question()` ‚Üí Generates answers from FLAN-T5  

"""

import gradio as gr

# üí† Light Blue Theme
custom_theme = gr.themes.Soft(
    primary_hue="blue",
    secondary_hue="gray",
).set(
    body_background_fill="#E8F1FA",
    body_text_color="#1A1A1A",
)

def process_notes(file):
    global note_texts, embeddings, faiss_index
    text = extract_text_from_file(file.name)
    note_texts = [text[i:i+300] for i in range(0, len(text), 300)]
    embeddings = embedder.encode(note_texts)
    embeddings = np.array(embeddings).astype("float32")
    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
    faiss_index.add(embeddings)
    return f"‚úÖ Processed {len(note_texts)} chunks successfully!"

def ask_question(query, history):
    if not query.strip():
        return history
    answer = generate_answer(query)
    history.append((query, answer))
    return history

with gr.Blocks(theme=custom_theme, title="üìò AI Notes Chatbot") as app:
    gr.Markdown("## ü§ñ AI Notes Chatbot ‚Äì Ask from Your Uploaded Notes")
    with gr.Row():
        with gr.Column(scale=1):
            upload = gr.File(label="Upload Notes (.txt / .pdf / .docx)")
            process_btn = gr.Button("‚öôÔ∏è Process Notes")
            status = gr.Textbox(label="Status", interactive=False)
        with gr.Column(scale=2):
            chatbot = gr.Chatbot(label="Chat Window")
            query = gr.Textbox(label="Type your question")
            send_btn = gr.Button("üí¨ Ask Question")

    process_btn.click(process_notes, inputs=upload, outputs=status)
    send_btn.click(ask_question, inputs=[query, chatbot], outputs=chatbot)

app.launch(share=True)